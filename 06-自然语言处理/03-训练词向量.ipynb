{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒè¯å‘é‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## åˆ©ç”¨`gensim.models.word2vec`è®­ç»ƒè¯å‘é‡\n",
    "- åŸå§‹è¯­æ–™ä¸º[ä¸­æ–‡ç»´åŸº](https://dumps.wikimedia.org/zhwiki/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. åŸå§‹è¯­æ–™ä¸º `xml` æ ¼å¼ï¼Œéœ€è¦æå–å‡ºæ­£æ–‡ï¼Œä½¿ç”¨ `WikiExtractor` åŒ…\n",
    "    0. å‘½ä»¤è¡Œæå–æ­£æ–‡ï¼š`python WikiExtractor.py -b 500M -o wiki zhwiki-20190720-pages-articles-multistream.xml.bz2`\n",
    "    0. è·å¾—çš„æ–‡ä»¶ä¸­ï¼Œæ­£æ–‡è¢«åŒ…å«åœ¨ `<doc></doc>` æ ‡ç­¾å†…\n",
    "0. æˆ–è€… `gensim.corpora.WikiCorpus` ç›´æ¥å¤„ç† `xml.bz2` æ–‡ä»¶\n",
    "0. ç”±ä¸Šä¸¤æ­¥ï¼Œè·å¾—çš„æ–‡æœ¬å…ˆç»è¿‡é¢„å¤„ç†ï¼Œ**æ¯ä¸€è¡Œä¸€å¥è¯ï¼Œå•è¯é—´ç”¨ç©ºæ ¼éš”å¼€**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python WikiExtractor.py -b 500M -o datasets/wiki datasets/zhwiki-20190720.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_zhwiki_v1():\n",
    "    # æå–æ–‡æœ¬ä¿¡æ¯ï¼Œåˆ†å¥ã€åˆ†è¯ã€ç¹ä½“è½¬ç®€ä½“ï¼Œç„¶åå°†å•è¯ç”¨ç©ºæ ¼è¿æ¥\n",
    "    regex = re.compile(\"(^<doc.*>$)|(^</doc>$)\")\n",
    "    sent_spliter = re.compile(\"ã€‚|ï¼|ï¼Ÿ\")\n",
    "\n",
    "    input_file = open(input_file_path, 'r', encoding='utf-8')\n",
    "    output_file = open(output_file_path, 'w+', encoding='utf-8')\n",
    "\n",
    "    line = input_file.readline()\n",
    "    while line:\n",
    "        if line.strip() and not regex.match(line):\n",
    "            sentences = sent_spliter.split(line)\n",
    "            for s in sentences:\n",
    "                s = zhconv.convert(s, 'zh-cn')\n",
    "                words = jieba.cut(s.strip('\\n'))\n",
    "                sent = ' '.join(words)\n",
    "                output_file.write(sent + '\\n')\n",
    "        line = input_file.readline()\n",
    "\n",
    "    input_file.close()\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "\n",
    "def preprocess_zhwiki_v2():\n",
    "    # æå–æ–‡æœ¬ä¿¡æ¯ï¼Œåˆ†å¥ã€åˆ†è¯ã€ç¹ä½“è½¬ç®€ä½“ï¼Œç„¶åå°†å•è¯ç”¨ç©ºæ ¼è¿æ¥\n",
    "    # WikiCorpus ä¼šå°†æ ‡ç‚¹ç¬¦å·éƒ½è¢«åˆ é™¤\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    l = []\n",
    "\n",
    "    output_file = open(output_file_path, 'w+', encoding='utf-8')\n",
    "\n",
    "    wiki = WikiCorpus(input_file_path, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        for temp_sentence in text:\n",
    "            temp_sentence = zhconv.convert(s, 'zh-cn')\n",
    "            seg_list = list(jieba.cut(temp_sentence))\n",
    "            for temp_term in seg_list:\n",
    "                l.append(temp_term)\n",
    "        output_file.write(space.join(l) + '\\n')\n",
    "        l = []\n",
    "        i = i + 1\n",
    "\n",
    "        if (i % 200 == 0):\n",
    "            print('Saved ' + str(i) + ' articles')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "input_file_path = r'datasets/wiki/AA/wiki_00'\n",
    "output_file_path = r'datasets/wiki/AA/wiki_corpus'\n",
    "preprocess_zhwiki_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- åˆ©ç”¨ä¸Šä¸€æ­¥ç”Ÿæˆçš„å¤„ç†åçš„æ»¡è¶³ `LineSentence` æ ¼å¼çš„æ–‡æœ¬ï¼Œåˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "corpus_path = output_file_path\n",
    "model_path = r\"models/wiki_corpus.model\"\n",
    "\n",
    "\n",
    "def build_model(corpus_path):\n",
    "    wiki_news = word2vec.LineSentence(corpus_path)\n",
    "    model = word2vec.Word2Vec(\n",
    "        wiki_news,\n",
    "        sg=0,  # æ¨¡å‹ç±»å‹ CBOW\n",
    "        size=50,  # è¯å‘é‡ç»´åº¦     \n",
    "        window=5,  # çª—å£å°ºå¯¸\n",
    "        min_count=5, # å¿½ç•¥è¯é¢‘å°‘äº 5 çš„å•è¯\n",
    "        workers=9)\n",
    "    model.save(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- éªŒè¯è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model_path = r\"models/zhwiki.50d.word2vec\"\n",
    "model = word2vec.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('æ•°å­¦')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('å“²å­¦')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['å¥³äºº', 'å›½ç‹'], negative=['ç”·äºº'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_corpus = [\"è…¾è®¯\",\"é˜¿é‡Œå·´å·´\"]\n",
    "res = model.wv.similarity(two_corpus[0],two_corpus[1])\n",
    "print(\"similarity:%.4f\"%res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å°†è¯å‘é‡é™ç»´åè¿›è¡Œå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "%matplotlib inline\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "\n",
    "def get_model_matrix(word_vectors, required_words):\n",
    "    import random\n",
    "    words = list(word_vectors.vocab.keys())\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(word_vectors.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        try:\n",
    "            M.append(word_vectors.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind\n",
    "\n",
    "\n",
    "words = [\n",
    "    'æ•°å­¦', 'ç®—æœ¯', 'å…¬ç†', 'ç§¯åˆ†', 'ç»Ÿè®¡', 'å–„æ¶', 'å“²å­¦', 'ä¼¦ç†', 'ä¸­å›½æ”¿åºœ', 'ç¾å›½å›½ä¼š', 'æ­¦ä¾ å°è¯´',\n",
    "    'é£é¡', 'æµ·å†…å¤–', 'å—æ¬¢è¿', 'é€šä¿—å°è¯´', 'ä¸­åäººæ°‘å…±å’Œå›½', 'æ–‡åŒ–å¤§é©å‘½', 'åæ€', 'ä¼¤ç—•', 'ä¸€æ‰¹', 'ç™½è¯æ–‡',\n",
    "    'è¯—äºº', 'å¤è¯—', 'æ¬¢è¿', 'ä¸­åæ°‘å›½', 'æ’¤é€€', 'å°æ¹¾', 'åŒºåˆ«', 'æ€æ½®', 'è¿‡æ¸¡æ—¶æœŸ', 'é€šç§°', 'æ–‡çŒ®', 'å…´è¶£',\n",
    "    'é’»ç ”', 'è¯­è¨€å­¦', 'ç¥ç§˜ä¸»ä¹‰', 'æ›´åŠ ', 'ç»å…¸', 'å†å²å­¦', 'æ–‡å­¦', 'å­¦æœ¯ç•Œ', 'äº«æœ‰', 'å‰æ‰€æœªæœ‰', 'è¶‹åŠ¿',\n",
    "    'å—åˆ°', 'äººæ–‡ä¸»ä¹‰è€…', 'å·¨é‡', 'è§„åˆ™', 'æœºå™¨äºº', 'ç²¾å‡†', 'èº«èº¯', 'è„‘', 'è§†é¢‘', 'ç¡®ä¿', 'é«˜è´¨é‡', 'é€‚ä¸­',\n",
    "    'ä»·æ ¼', 'è½¯ä»¶è®¾è®¡', 'æ„æˆ', 'äº’è¡¥', 'å¹¶è¡Œ', 'ç³»ç»Ÿåˆ†æ', 'ç¨‹åºè®¾è®¡', 'æ”¯æŒ', 'é«˜çº§', 'è¯¾ç¨‹', 'è®­ç»ƒ',\n",
    "    'å·¥ä¸š', 'æŠ€èƒ½', 'ç¾§é…¸', 'æŸ æª¬é…¸', 'é«˜æ•ˆç‡', 'è‚½é”®', 'ç»†èƒéª¨æ¶', 'ç»†èƒå‘¨æœŸ', 'æ°¯ä»¿', 'ç”˜æ²¹', 'å˜å‹',\n",
    "    'é˜', 'ç±»å›ºé†‡', 'é†›', 'é…®', 'ç³–åŸ', 'å•ç³–', 'åŠä¹³ç³–', 'è‘¡è„ç³–', 'ç³–è‹·é”®', 'å«æ°®', 'æ‚ç¯', 'å˜Œå‘¤',\n",
    "    'è¾…é…¶', 'åº•ç‰©', 'åŒ–å­¦èƒ½', 'ç£·é…¸åŒ–', 'å“ˆåº·', 'å»¶æ–¯', 'æŒªå¨æµ·', 'æ•é²¸', 'æŒªå¨æ”¿åºœ', 'æˆäººç¤¼', 'å·´ä¼¦æ”¯æµ·',\n",
    "    'å“¥å¾·å ¡', 'åŒºåŸŸè§„åˆ’', 'æ¶¦å·', 'é‚³å·å¸‚', 'ä¸œæµ·å¿', 'ä¸¹é˜³å¸‚', 'æ­¦è¿›åŒº', 'ä¸´æ²³', 'å˜ˆæ‚', 'éœ°å¼¹æª', 'è®²å¸­',\n",
    "    'ä¸€æ»´', 'è°ƒæ¢', 'é¦™æ¸¯é‡‘èç®¡ç†å±€', 'ç¾åœ†', 'é‡‘ç®¡å±€', 'æ¯«', 'å¤§é¢', 'é“œå¸', 'ä¸€åœ†', 'é•å¸', 'çˆ†ç«¹',\n",
    "    'ç®¡ç†ç§‘', 'ä¸­åŒº', 'æ”¶å…‘', 'è´¢æ”¿å¸'\n",
    "]\n",
    "\n",
    "M, word2Ind = get_model_matrix(word_vectors, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ©ç”¨ svd ç®—æ³•è¿›è¡Œé™ç»´\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    n_iters = 10\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "M_reduced = reduce_to_k_dim(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(24,24))\n",
    "    for word in words:\n",
    "        index = word2Ind[word]\n",
    "        x, y = M_reduced[index]\n",
    "        plt.scatter(x, y, marker='o', color='red')\n",
    "        plt.text(x, y, word, fontsize=9)\n",
    "        \n",
    "plot_embeddings(M_reduced, word2Ind, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# åˆ©ç”¨ TSNE ç®—æ³•è¿›è¡Œé™ç»´\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_plot(M, word2Ind, words):\n",
    "\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    M_reduced = tsne_model.fit_transform(M)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(32, 32))\n",
    "    for word in words:\n",
    "        index = word2Ind[word]\n",
    "        x, y = M_reduced[index]\n",
    "        ax.scatter(x, y, marker='o', color='red')\n",
    "        ax.text(x, y, word, fontsize=9)\n",
    "        \n",
    "tsne_plot(M, word2Ind, words)\n",
    "\n",
    "# TSNE é™ç»´æ•ˆæœæ¯” SVD è¦å¥½ï¼Œä½†æ•ˆç‡æ›´ä½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å…³é”®è¯æå–ï¼Œä» `wv.most_similar()` å‡ºå‘è·å–ç»™å®šå•è¯çš„ç›¸å…³å•è¯\n",
    "    - è¯å‘é‡ `wv.most_similar()` è·å¾—çš„ä¸ºå‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„åŒç±»è¯ï¼Œå¹¶ä¸æ˜¯é€šå¸¸è¯­ä¹‰å«ä¹‰ä¸Šçš„ç›¸ä¼¼è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_related_words(initial_words, model):\n",
    "    unseen = [initial_words]\n",
    "    seen = defaultdict(int)\n",
    "\n",
    "    max_size = 500\n",
    "\n",
    "    while unseen and len(seen) < max_size:\n",
    "        if len(seen) % 50 == 0:\n",
    "            print('search length: {}'.format(len(seen)))\n",
    "\n",
    "        node = unseen.pop(0)\n",
    "        new_expanding = [w for w, _ in model.most_similar(node, topn=20)]\n",
    "        unseen += new_expanding\n",
    "\n",
    "        seen[node] += 1\n",
    "    return seen\n",
    "\n",
    "\n",
    "actions = get_related_words(\"è¯´\", word_vectors)\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `wordcloud` å®ç°è¯äº‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ¹æ ¹æ•£æ–‡é›†çš„è¯äº‘\n",
    "data_path = r'datasets/Bacon Francis - Essays.txt'\n",
    "\n",
    "import os\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = open(data_path).read()\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tensorflow.nn.sampled_softmax_loss` è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- åˆ©ç”¨ä¸Šè¿°é¢„å¤„ç†åçš„æ–‡ä»¶\n",
    "- éœ€è¦æ ¹æ®è¯­æ–™åˆ›å»ºè¯æ±‡è¡¨\n",
    "- éœ€è¦å°†å¥å­åˆ†è¯åçš„è¯è¯­åˆ—è¡¨ï¼Œè½¬å˜æˆ ä¸­å¿ƒè¯-ä¸Šä¸‹æ–‡è¯ ç»„æˆçš„è¯å¯¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T10:04:53.852955Z",
     "start_time": "2020-04-08T10:04:52.641593Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import bz2\n",
    "\n",
    "import random\n",
    "import math\n",
    "from six.moves import xrange\n",
    "\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T10:06:37.547294Z",
     "start_time": "2020-04-08T10:06:35.964139Z"
    }
   },
   "outputs": [],
   "source": [
    "# å°†åŸå§‹æ–‡ä»¶è½¬åŒ–æˆå•è¯åˆ—è¡¨\n",
    "# å¤§æ®µçš„æ–‡æœ¬ç›´æ¥åˆ©ç”¨ç©ºæ ¼æ‹†åˆ†\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_text(f.read(f.namelist()[0])).split()\n",
    "    return words\n",
    "\n",
    "\n",
    "filename = r'datasets/text8.zip'\n",
    "vocabulary = read_data(filename)\n",
    "\n",
    "# æ–‡æœ¬ä¸­æ€»å•è¯é‡\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# è¯æ±‡è¡¨å¤§å°\n",
    "len(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T10:08:23.788429Z",
     "start_time": "2020-04-08T10:08:20.825628Z"
    }
   },
   "outputs": [],
   "source": [
    "# åˆ›å»ºè¯å…¸ï¼Œæ›¿æ¢ç¨€æœ‰è¯\n",
    "vocabulary_size = 10000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    # å•è¯-è¯é¢‘\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "\n",
    "    # è¯å…¸ï¼šå•è¯-ç´¢å¼•\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    # æ–‡æœ¬å‘é‡åŒ–\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "\n",
    "    # ç´¢å¼•-å•è¯ï¼Œè¯å…¸\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "del vocabulary\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ç”Ÿæˆæ‰¹å¤„ç†æ•°æ®\n",
    "    - å°†æ–‡æœ¬`the quick brown fox jumped over the lazy dog`è½¬æ¢ä¸ºæ•°ç»„ $[123,6,53,24,5,12,89,8,11]$\n",
    "    - $skip\\_window = 2$ï¼Œå¯¹äºä¸­å¿ƒè¯ `53` åˆ™ä¸Šä¸‹æ–‡-ä¸­å¿ƒè¯å¯¹$ [(123ï¼Œ53),(6ï¼Œ53),(24ï¼Œ53),(5,53]$\n",
    "    - $num\\_skips = 3 $ï¼Œä»æ¯ä¸ªçª—å£è¯å¯¹ä¸­é€‰æ‹© 3 ä¸ªï¼Œå¦‚ $[(123,53),(24,53),(5,53)]$\n",
    "    - éå†æ‰€æœ‰ä¸­å¿ƒè¯ï¼Œå°†è¯å¯¹è½¬åŒ–æˆè§£åŒ…ï¼Œå¦‚ batch ä¸ºä¸­å¿ƒè¯æ•°ç»„ $[53,53,53]$ï¼Œlabels ä¸ºä¸Šä¸‹æ–‡è¯æ•°ç»„ $[123, 24, 5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç”¨äº skip-gram æ¨¡å‹çš„è®­ç»ƒæ•°æ®\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_dictionary[i] for i in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹å‚æ•°\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 64  # è¯å‘é‡çš„ç»´åº¦\n",
    "skip_window = 1  # çª—å£å¤§å°\n",
    "num_skips = 2  # æ¯ä¸ªçª—å£çš„è¯å¯¹ä¸­é€‰æ‹©å¤šå°‘å¯¹\n",
    "num_sampled = 64  # è´Ÿé‡‡æ ·æ—¶è´Ÿæ ·æœ¬æ•°é‡\n",
    "\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹ï¼Œtf.nn.sampled_softmax_loss\n",
    "initializer_softmax = tf.keras.initializers.GlorotUniform()\n",
    "# Variables:\n",
    "embeddings_weight = tf.Variable(\n",
    "    tf.random.uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weight = tf.Variable(\n",
    "    initializer_softmax([vocabulary_size, embedding_size]))\n",
    "softmax_bias = tf.Variable(initializer_softmax([vocabulary_size]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "num_step = 100001\n",
    "for step in range(num_step):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                skip_window)\n",
    "    batch_inputs = tf.cast(batch_inputs, tf.int32)\n",
    "    batch_labels = tf.cast(batch_labels, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        embed = tf.nn.embedding_lookup(embeddings_weight, batch_inputs)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(weights=softmax_weight,\n",
    "                                       biases=softmax_bias,\n",
    "                                       inputs=embed,\n",
    "                                       labels=batch_labels,\n",
    "                                       num_sampled=num_sampled,\n",
    "                                       num_classes=vocabulary_size))\n",
    "    variables = [embeddings_weight, softmax_weight, softmax_bias]\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:47:22.596016Z",
     "start_time": "2020-04-08T12:47:22.590897Z"
    }
   },
   "source": [
    "## `tensorflow.nn.nce_loss` è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹\n",
    "batch_size = 64\n",
    "vocabulary_size = len(word2ind)\n",
    "embedding_dimension = 5\n",
    "negative_samples = 8\n",
    "LOG_DIR = 'logs/word2vec'\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random.uniform([vocabulary_size, embedding_dimension], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(\n",
    "    tf.random.truncated_normal([vocabulary_size, embedding_dimension],\n",
    "                               stddev=1.0 / math.sqrt(embedding_dimension)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "variables = [embeddings, nce_weights, nce_weights]\n",
    "\n",
    "\n",
    "def loss_fn(embed, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       inputs=embed,\n",
    "                       labels=labels,\n",
    "                       num_sampled=negative_samples,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for i in range(1000):\n",
    "    x_batch, y_batch = generate_batch(batch_size)\n",
    "    x_batch = tf.constant(x_batch, dtype=tf.int32)\n",
    "    y_batch = tf.constant(y_batch, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, x_batch)\n",
    "        loss = loss_fn(embed, y_batch)\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tensorflow`è´Ÿé‡‡æ ·è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:05.981831Z",
     "start_time": "2020-04-08T12:31:05.165551Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T10:13:32.234593Z",
     "start_time": "2020-04-08T10:13:32.226666Z"
    }
   },
   "source": [
    "### å¤„ç†æ•°æ®é›†\n",
    "åå°”è¡—æ—¥æŠ¥çš„æ–‡ç« ï¼Œæ¯ä¸€è¡Œä¸€ä¸ªå¥å­ï¼Œå¥å­ä¸­æ¯ä¸ªè¯ç”±ç©ºæ ¼éš”å¼€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:06.088490Z",
     "start_time": "2020-04-08T12:31:05.991310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim',\n",
       " 'snack-food',\n",
       " 'ssangyong',\n",
       " 'swapo',\n",
       " 'wachter']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir('datasets/ptb')\n",
    "\n",
    "# å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªå¥å­ä¸ºå•è¯ç»„æˆçš„åˆ—è¡¨\n",
    "with open('datasets/ptb/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "print(len(raw_dataset))\n",
    "raw_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:09.987423Z",
     "start_time": "2020-04-08T12:31:09.981617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('# tokens:', len(st), st[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å»ºç«‹è¯è¯­ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:10.979899Z",
     "start_time": "2020-04-08T12:31:10.900348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9858, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç»Ÿè®¡å•è¯å‡ºç°çš„é¢‘æ¬¡ï¼Œåˆ é™¤è¯é¢‘<5çš„å•è¯\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))\n",
    "len(counter), counter['learn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:11.715725Z",
     "start_time": "2020-04-08T12:31:11.617748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åˆ›å»ºå•è¯ä¸ç´¢å¼•æ˜ å°„\n",
    "idx2token = [tk for tk, _ in counter.items()]\n",
    "token2idx = {tk: idx for idx, tk in enumerate(idx2token)}\n",
    "\n",
    "# å°†æ•°æ®é›†è½¬åŒ–ä¸ºç´¢å¼•åˆ—è¡¨\n",
    "dataset = [[token2idx[tk] for tk in st if tk in token2idx]\n",
    "           for st in raw_dataset]\n",
    "\n",
    "# è¯­æ–™æ€»å•è¯æ ‘\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äºŒæ¬¡é‡‡æ ·\n",
    "æ–‡æœ¬æ•°æ®ä¸­ä¸€èˆ¬ä¼šå‡ºç°ä¸€äº›é«˜é¢‘è¯ï¼Œå¦‚è‹±æ–‡ä¸­çš„â€œtheâ€â€œaâ€å’Œâ€œinâ€ï¼Œåœ¨ä¸€ä¸ªèƒŒæ™¯çª—å£ä¸­ï¼Œä¸€ä¸ªè¯ï¼ˆå¦‚â€œchipâ€ï¼‰å’Œè¾ƒä½é¢‘è¯ï¼ˆå¦‚â€œmicroprocessorâ€ï¼‰åŒæ—¶å‡ºç°æ¯”å’Œè¾ƒé«˜é¢‘è¯ï¼ˆå¦‚â€œtheâ€ï¼‰åŒæ—¶å‡ºç°å¯¹è®­ç»ƒè¯åµŒå…¥æ¨¡å‹æ›´æœ‰ç›Šã€‚å› æ­¤ï¼Œè®­ç»ƒè¯åµŒå…¥æ¨¡å‹æ—¶å¯ä»¥å¯¹è¯è¿›è¡ŒäºŒæ¬¡é‡‡æ ·ã€‚ å…·ä½“æ¥è¯´ï¼Œæ•°æ®é›†ä¸­æ¯ä¸ªè¢«ç´¢å¼•è¯$w_i$å°†æœ‰ä¸€å®šæ¦‚ç‡è¢«ä¸¢å¼ƒï¼Œè¯¥ä¸¢å¼ƒæ¦‚ç‡ä¸º\n",
    "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right)$$\n",
    "å…¶ä¸­ $f(w_i)$ æ˜¯æ•°æ®é›†ä¸­è¯$w_i$çš„ä¸ªæ•°ä¸æ€»è¯æ•°ä¹‹æ¯”ï¼Œå¸¸æ•°$t$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼ˆå®éªŒä¸­è®¾ä¸º$10^{-4}$ï¼‰ã€‚å¯è§ï¼Œåªæœ‰å½“$f(w_i) > t$æ—¶ï¼Œæˆ‘ä»¬æ‰æœ‰å¯èƒ½åœ¨äºŒæ¬¡é‡‡æ ·ä¸­ä¸¢å¼ƒè¯$w_i$ï¼Œå¹¶ä¸”è¶Šé«˜é¢‘çš„è¯è¢«ä¸¢å¼ƒçš„æ¦‚ç‡è¶Šå¤§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:14.367013Z",
     "start_time": "2020-04-08T12:31:13.970089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375402'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ˜¯å¦ä¸¢å¼ƒå•è¯ï¼Œè¯¥å•è¯åœ¨è¯­æ–™ä¸­å‡ºç°çš„æ€»æ¬¡æ•°è¶Šé«˜ï¼Œè¶Šå¯èƒ½è¢«ä¸¢å¼ƒ\n",
    "def discard(idx):\n",
    "    return random.uniform(\n",
    "        0, 1) < 1 - math.sqrt(1e-4 / counter[idx2token[idx]] * num_tokens)\n",
    "\n",
    "# é‡é‡‡æ ·åæ•°æ®é›†\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "\n",
    "# è¯­æ–™æ€»å•è¯æ•°å¤§å¤§å‡å°‘\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:16.978815Z",
     "start_time": "2020-04-08T12:31:16.940305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# the: before=50770, after=2114\n",
      "# join: before=45, after=45\n"
     ]
    }
   ],
   "source": [
    "# äºŒæ¬¡é‡‡æ ·åï¼Œå•è¯è¢«ä¿ç•™çš„æ¬¡æ•°\n",
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (\n",
    "        token, sum([st.count(token2idx[token]) for st in dataset]),\n",
    "        sum([st.count(token2idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "\n",
    "print(compare_counts('the'))\n",
    "# é«˜é¢‘è¯theè¢«ä¿ç•™çš„æ¬¡æ•°ä» 50770 é™ä½åˆ° 2153\n",
    "print(compare_counts('join'))\n",
    "# ä½é¢‘è¯joinåŸºæœ¬éƒ½ä¿ç•™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æå–ä¸­å¿ƒè¯å’ŒèƒŒæ™¯è¯\n",
    "æ¯æ¬¡åœ¨æ•´æ•°1å’Œmax_window_sizeï¼ˆæœ€å¤§èƒŒæ™¯çª—å£ï¼‰ä¹‹é—´éšæœºå‡åŒ€é‡‡æ ·ä¸€ä¸ªæ•´æ•°ä½œä¸ºèƒŒæ™¯çª—å£å¤§å°ã€‚æ¯ä¸ªä¸­å¿ƒè¯ï¼Œå¯¹åº”ä¸€ä¸ªèƒŒæ™¯è¯åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:18.073395Z",
     "start_time": "2020-04-08T12:31:18.067059Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¯ä¸ªä¸­å¿ƒè¯å¯¹åº”ä¸€ä¸ªèƒŒæ™¯è¯åˆ—è¡¨\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # æ¯ä¸ªå¥å­è‡³å°‘è¦æœ‰2ä¸ªè¯æ‰å¯èƒ½ç»„æˆä¸€å¯¹â€œä¸­å¿ƒè¯-èƒŒæ™¯è¯â€\n",
    "            continue\n",
    "        centers += st  # æ¯å¥è¯æ‰€æœ‰å•è¯éƒ½æ˜¯ä¸­å¿ƒè¯\n",
    "        for i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(\n",
    "                range(max(0, i - window_size), min(len(st),\n",
    "                                                   i + window_size + 1)))\n",
    "            indices.remove(i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:18.892452Z",
     "start_time": "2020-04-08T12:31:18.885003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4], [8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4]\n",
      "center 4 has contexts [3]\n",
      "center 8 has contexts [9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(5)), list(range(8, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:22.551101Z",
     "start_time": "2020-04-08T12:31:21.754140Z"
    }
   },
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è´Ÿé‡‡æ ·\n",
    "- å¸¸è§„çš„$softmax$ä¼šè¾“å‡ºæ•´ä¸ªè¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè®¡ç®—é‡å·¨å¤§\n",
    "- ä½¿ç”¨è´Ÿé‡‡æ ·æ¥è¿›è¡Œè¿‘ä¼¼è®­ç»ƒï¼šå¯¹äºä¸€å¯¹ä¸­å¿ƒè¯å’ŒèƒŒæ™¯è¯ï¼Œæˆ‘ä»¬éšæœºé‡‡æ ·$K$ä¸ªå™ªå£°è¯ï¼ˆå®éªŒä¸­è®¾$K=5$ï¼‰ã€‚æ ¹æ®word2vecè®ºæ–‡çš„å»ºè®®ï¼Œå™ªå£°è¯é‡‡æ ·æ¦‚ç‡$P(w)$è®¾ä¸º$w$è¯é¢‘ä¸æ€»è¯é¢‘ä¹‹æ¯”çš„0.75æ¬¡æ–¹ã€‚\n",
    "- é‡‡æ ·ä¹‹åï¼Œä¸€ä¸ªä¸­æ€§è¯å¯¹åº”ä¸€ä¸ªèƒŒæ™¯è¯åˆ—è¡¨ï¼Œè¿˜å¯¹åº”ä¸€ä¸ªå™ªå£°è¯åˆ—è¡¨ï¼Œå™ªå£°è¯åˆ—è¡¨çš„é•¿åº¦æ˜¯èƒŒæ™¯è¯åˆ—è¡¨é•¿åº¦çš„$K$å€ï¼Œæ­¤æ—¶$sofmax$è¾“å‡ºä¸ºèƒŒæ™¯è¯+å™ªå£°è¯åˆ—è¡¨çš„æ¦‚ç‡åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:29.268617Z",
     "start_time": "2020-04-08T12:31:23.332296Z"
    }
   },
   "outputs": [],
   "source": [
    "# ä»¥è¯é¢‘çš„0.75æ¬¡æ–¹ä½œä¸ºæƒé‡è¿›è¡Œé‡‡æ ·\n",
    "sampling_weights = [counter[w]**0.75 for w in idx2token]\n",
    "\n",
    "\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # å¾ªç¯å¼€å§‹æ—¶ï¼Œæ ¹æ®æ¯ä¸ªè¯çš„æƒé‡ï¼ˆsampling_weightsï¼‰éšæœºç”Ÿæˆkä¸ªè¯çš„ç´¢å¼•ä½œä¸ºå€™é€‰å™ªå£°è¯ã€‚\n",
    "                # ä¸ºäº†é«˜æ•ˆè®¡ç®—ï¼Œå¯ä»¥å°†kè®¾å¾—ç¨å¤§ä¸€ç‚¹\n",
    "                i, neg_candidates = 0, random.choices(population,\n",
    "                                                      sampling_weights,\n",
    "                                                      k=int(1e5))\n",
    "            # ä¾æ¬¡å°†å€™é€‰å™ªå£°è¯æ·»åŠ åˆ°å™ªå£°è¯ä¸­\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # å™ªå£°è¯ä¸èƒ½æ˜¯èƒŒæ™¯è¯\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:31:31.361295Z",
     "start_time": "2020-04-08T12:31:31.327317Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¯ä¸ªä¸­å¿ƒè¯å¯¹åº”çš„å™ªå£°è¯åˆ—è¡¨çš„é•¿åº¦ï¼Œæ˜¯èƒŒæ™¯è¯åˆ—è¡¨é•¿åº¦çš„ğ¾å€\n",
    "assert all([\n",
    "    len(neg) == len(window) * 5\n",
    "    for window, neg in zip(all_contexts, all_negatives)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®ç®¡é“\n",
    "- æ¯ä¸ªå•è¯ï¼Œå¯¹åº”ä¸€ä¸ª**ç›¸åŒé•¿åº¦**çš„ä¸Šä¸‹æ–‡è¯åˆ—è¡¨ï¼šä¸Šä¸‹æ–‡ç”±èƒŒæ™¯è¯+å™ªå£°è¯+å¡«å……ç»„æˆï¼›\n",
    "- masks è¡¨æ˜ä¸Šä¸‹æ–‡ä¸­å•è¯æ˜¯å¦æ˜¯å¡«å……ï¼›\n",
    "- labels è¡¨æ˜ä¸Šä¸‹æ–‡ä¸­çš„æ¯ä¸ªå•è¯æ˜¯ä¸æ˜¯èƒŒæ™¯è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:01.305519Z",
     "start_time": "2020-04-08T12:34:01.298323Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(centers, contexts, negatives):\n",
    "    max_len = max(\n",
    "        len(cont) + len(neg) for cont, neg in zip(contexts, negatives))\n",
    "    targets, masks, labels = [], [], []\n",
    "    for cont, neg in zip(contexts, negatives):\n",
    "        cur_len = len(cont) + len(neg)\n",
    "\n",
    "        # ç”¨ 0 å¡«å……æˆç›¸åŒçš„é•¿åº¦\n",
    "        targets.append(cont + neg + [0] * (max_len - cur_len))\n",
    "\n",
    "        # åŒºåˆ«å•è¯å’Œå¡«å……\n",
    "        masks.append([1] * cur_len + [0] * (max_len - cur_len))\n",
    "\n",
    "        # èƒŒæ™¯è¯æ‰æ˜¯æ ‡ç­¾ï¼Œå™ªå£°è¯å’Œå¡«å……éƒ½ä¸æ˜¯\n",
    "        labels.append([1] * len(cont) + [0] * (max_len - len(cont)))\n",
    "    centers = np.array(centers, dtype=np.float32).reshape(-1, 1)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "    masks = np.array(masks, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.float32)\n",
    "    return (centers, targets, masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:08.207501Z",
     "start_time": "2020-04-08T12:34:03.254115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 136 ms, total: 4.95 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = tf.data.Dataset.from_tensor_slices(preprocess(\n",
    "    all_centers, all_contexts, all_negatives))\n",
    "\n",
    "batch_size = 512\n",
    "dataset = dataset.shuffle(len(all_centers)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:10.968984Z",
     "start_time": "2020-04-08T12:34:10.133893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: (512, 1)\n",
      "contexts_negatives shape: (512, 60)\n",
      "masks shape: (512, 60)\n",
      "labels shape: (512, 60)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.take(1):\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'],\n",
    "                          batch):\n",
    "        print(name, 'shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:14.189613Z",
     "start_time": "2020-04-08T12:34:13.381274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: <dtype: 'float32'>\n",
      "contexts_negatives shape: <dtype: 'float32'>\n",
      "masks shape: <dtype: 'float32'>\n",
      "labels shape: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.take(1):\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'],\n",
    "                          batch):\n",
    "        print(name, 'shape:', data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skip-gram`æ¨¡å‹\n",
    "åˆ©ç”¨ä¸­å¿ƒè¯é¢„æµ‹(èƒŒæ™¯è¯+å™ªå£°è¯)çš„æ¦‚ç‡åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:18.009051Z",
     "start_time": "2020-04-08T12:34:17.995818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00248704,  0.04588217,  0.03903292,  0.02340596],\n",
       "        [ 0.01726497, -0.04898253,  0.03221582,  0.02792713],\n",
       "        [-0.03564789,  0.04589545, -0.04923713, -0.00122647],\n",
       "        [ 0.04300623,  0.03403026,  0.03677107,  0.02419565],\n",
       "        [ 0.01740289, -0.03229471,  0.00015046,  0.00863054],\n",
       "        [ 0.04152843,  0.04772225,  0.02492184,  0.01654983]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åµŒå…¥å±‚\n",
    "embed = tf.keras.layers.Embedding(input_dim=6, output_dim=4)\n",
    "embed.build(input_shape=(1, 6))\n",
    "embed.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:22.807498Z",
     "start_time": "2020-04-08T12:34:22.800852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=77, shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.01726497, -0.04898253,  0.03221582,  0.02792713],\n",
       "        [-0.03564789,  0.04589545, -0.04923713, -0.00122647],\n",
       "        [ 0.04300623,  0.03403026,  0.03677107,  0.02419565]],\n",
       "\n",
       "       [[ 0.01740289, -0.03229471,  0.00015046,  0.00863054],\n",
       "        [ 0.04152843,  0.04772225,  0.02492184,  0.01654983],\n",
       "        [ 0.00248704,  0.04588217,  0.03903292,  0.02340596]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [4, 5, 0]], dtype=tf.int32)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:24.736197Z",
     "start_time": "2020-04-08T12:34:24.733754Z"
    }
   },
   "outputs": [],
   "source": [
    "# è·³å­—æ¨¡å‹ï¼Œæ‰¹é‡æ•°æ®ï¼Œè¾“å‡ºæƒé‡å‘é‡\n",
    "@tf.function\n",
    "def skip_gram(centers, contexts, embed_v, embed_u):\n",
    "    v = embed_v(centers)\n",
    "    u = embed_u(contexts)\n",
    "    pred = tf.matmul(v, tf.transpose(u, perm=[0, 2, 1]))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T11:28:39.800967Z",
     "start_time": "2020-04-08T11:28:39.795864Z"
    }
   },
   "source": [
    "### è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æŸå¤±å‡½æ•°\n",
    "è´Ÿé‡‡æ ·åçš„æ•°æ®ä¸­å¼•å…¥äº†`mask`ï¼Œå¡«å……çš„æ•°æ®åœ¨è®­ç»ƒæ—¶åº”è¯¥è¢«é®è”½æ‰ï¼Œå¯¹åº”çš„æŸå¤±ä¸åº”è¯¥è®¡å…¥æ¨¡å‹æŸå¤±å‡½æ•°ã€‚æŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œä¸‹é¢å®šä¹‰`SigmoidBinaryCrossEntropyLoss`:`x = logits`, `z = labels`\n",
    "\n",
    "$$loss = z\\times(-log(sigmoid(x))) + (1 - z)\\times(-log(1 - sigmoid(x)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:32.947876Z",
     "start_time": "2020-04-08T12:34:32.941824Z"
    }
   },
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropy(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropy, self).__init__()\n",
    "\n",
    "    def __call__(self, labels, logits, mask=None):\n",
    "        res = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits) * mask\n",
    "        return tf.reduce_mean(res, axis=1)\n",
    "\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:38.122627Z",
     "start_time": "2020-04-08T12:34:37.753109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=98, shape=(2,), dtype=float32, numpy=array([0.8739896, 1.2099689], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tf.constant([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]],\n",
    "                   dtype=tf.float32)\n",
    "\n",
    "# æ ‡ç­¾å˜é‡labelä¸­çš„1å’Œ0åˆ†åˆ«ä»£è¡¨èƒŒæ™¯è¯å’Œå™ªå£°è¯\n",
    "label = tf.constant([[1, 0, 0, 0], [1, 1, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "# æ©ç å˜é‡\n",
    "mask = tf.constant([[1, 1, 1, 1], [1, 1, 1, 0]], dtype=tf.float32)\n",
    "\n",
    "loss(label, pred, mask) * mask.shape[1] / tf.reduce_sum(mask, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:34:51.391785Z",
     "start_time": "2020-04-08T12:34:51.370811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fd5165667d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç¬¬ä¸€ä¸ªåµŒå…¥å±‚ä¸ºéœ€è¦çš„è¯å‘é‡ï¼Œç¬¬äºŒä¸ªåµŒå…¥å±‚ä¸¢å¼ƒ\n",
    "embed_size = 100\n",
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(idx2token),\n",
    "                              output_dim=embed_size,\n",
    "                              name='word2vec'),\n",
    "    tf.keras.layers.Embedding(input_dim=len(idx2token),\n",
    "                              output_dim=embed_size,\n",
    "                              name='output')\n",
    "])\n",
    "net.get_layer(name='word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:35:01.622175Z",
     "start_time": "2020-04-08T12:35:01.614331Z"
    }
   },
   "outputs": [],
   "source": [
    "# è®­ç»ƒè¿‡ç¨‹\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        start, cur_loss, n = time.time(), 0.0, 0\n",
    "        for center, target, mask, label in dataset:\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                pred = skip_gram(center, target, net.get_layer(index=0),\n",
    "                                 net.get_layer(index=1))\n",
    "                # ä½¿ç”¨æ©ç å˜é‡maskæ¥é¿å…å¡«å……é¡¹å¯¹æŸå¤±å‡½æ•°è®¡ç®—çš„å½±å“\n",
    "                l = (loss(label, tf.reshape(pred, label.shape), mask) *\n",
    "                     mask.shape[1] / tf.reduce_sum(mask, axis=1))\n",
    "                l = tf.reduce_mean(l)  # ä¸€ä¸ªbatchçš„å¹³å‡loss\n",
    "\n",
    "            grads = tape.gradient(l, net.variables)\n",
    "            optimizer.apply_gradients(zip(grads, net.variables))\n",
    "            cur_loss += l.numpy().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' %\n",
    "              (epoch + 1, cur_loss / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:38:35.462984Z",
     "start_time": "2020-04-08T12:35:04.478511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.45, time 21.91s\n",
      "epoch 2, loss 0.39, time 21.23s\n",
      "epoch 3, loss 0.35, time 21.08s\n",
      "epoch 4, loss 0.32, time 20.95s\n",
      "epoch 5, loss 0.31, time 20.93s\n",
      "epoch 6, loss 0.30, time 20.90s\n",
      "epoch 7, loss 0.30, time 20.95s\n",
      "epoch 8, loss 0.29, time 20.91s\n",
      "epoch 9, loss 0.29, time 21.21s\n",
      "epoch 10, loss 0.28, time 20.91s\n"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### éªŒè¯æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T12:39:41.592862Z",
     "start_time": "2020-04-08T12:39:41.579251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.534: microprocessors\n",
      "cosine sim=0.505: dell\n",
      "cosine sim=0.498: micro\n",
      "cosine sim=0.457: folk\n",
      "cosine sim=0.447: intel\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.get_weights()\n",
    "    W = tf.convert_to_tensor(W[0])\n",
    "    x = W[token2idx[query_token]]\n",
    "    x = tf.reshape(x, shape=[-1, 1])\n",
    "    # æ·»åŠ çš„1e-9æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§\n",
    "    cos = tf.reshape(tf.matmul(W, x), shape=[\n",
    "        -1\n",
    "    ]) / tf.sqrt(tf.reduce_sum(W * W, axis=1) * tf.reduce_sum(x * x) + 1e-9)\n",
    "    _, topk = tf.math.top_k(cos, k=k + 1)\n",
    "    topk = topk.numpy().tolist()\n",
    "    for i in topk[1:]:  # é™¤å»è¾“å…¥è¯\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx2token[i])))\n",
    "\n",
    "\n",
    "get_similar_tokens('chip', 5, net.get_layer(index=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "879.85px",
    "left": "433px",
    "right": "20px",
    "top": "120px",
    "width": "358.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
